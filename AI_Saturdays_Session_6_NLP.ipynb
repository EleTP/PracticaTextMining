{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "AI Saturdays: Session 6 - NLP",
      "provenance": [],
      "collapsed_sections": [
        "MF5urIC3ExM7",
        "fhCytW1rExNR",
        "ckFN0ro3-jpU",
        "NwIN1bKzExPW",
        "IzQOTuiYyLPI",
        "ZimydDNmyNu4",
        "mv9sxHwx_LWu",
        "ezvHsteYExQr"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleTP/PracticaTextMining/blob/master/AI_Saturdays_Session_6_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-MxuvhYExM5",
        "colab_type": "text"
      },
      "source": [
        "# AI Saturdays: Session 6 - NLP\n",
        "\n",
        "Session 6 - **Natural Language Processing**\n",
        "\n",
        "Valencia - 16/11/2019\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![Saturdays logo](https://cdn-images-1.medium.com/max/718/1*e-i4CFTO6-ypIXccycTEBg@2x.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph7GneA-FX4U",
        "colab_type": "text"
      },
      "source": [
        "## 0.1. Objetivo\n",
        "\n",
        "- Comprender las aproximaciones al **Procesamiento del Lenguaje Natural (NLP)** basadas en Machine Learning.\n",
        "    - Análisis del funcionamiento de los algoritmos\n",
        "    - Extracción de características: Paso de datos no estructurados a datos ML-ready\n",
        "- Análisis de nuevos casos de negocio que abre el NLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQlesPPRFHtF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 0.2. About me\n",
        "Pablo González Carrizo ([@unmonoqueteclea](https://twitter.com/unmonoqueteclea/))\n",
        "\n",
        "- Mi foto más reciente:\n",
        "\n",
        "<center>\n",
        "<img src=https://unmonoqueteclea.github.io/assets/images/pequeno2.jpg width=\"300\">\n",
        "</center>\n",
        "\n",
        "\n",
        "- MSc in **Telecomunications Engineering**\n",
        "- Democratizing Machine Learning, as a **Machine Learning Engineer**, at [BigML](https://bigml.com/)\n",
        "\n",
        "<center>\n",
        "<img src=https://static.bigml.com/static/img/bigml.png width=\"200\">\n",
        "</center>\n",
        "\n",
        "Info and contact:\n",
        "  - https://unmonoqueteclea.github.io\n",
        "  - pgonzalezcarrizo@gmail.com\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr5yKKxpG_yu",
        "colab_type": "text"
      },
      "source": [
        "## 0.3. Programa\n",
        "\n",
        "\n",
        "### **Spoiler 1**\n",
        "\n",
        "Antes de almorzar tendremos funcionando un sistema capaz de analizar si un comentario sobre una película en IMDB es positivo o negativo con una exactitud cercana al 90%.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=https://i.gifer.com/V8zA.gif width=\"400\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "### **Spoiler 2**\n",
        "\n",
        "Al final habrá competición y **regalos** para los mejores\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=https://media.giphy.com/media/kKo2x2QSWMNfW/giphy.gif width=\"400\">\n",
        "</center>\n",
        "\n",
        "### Hablaremos de...\n",
        "- Sentiment Analysis with **IMDB** corpus and **Logistic Regression**\n",
        "- Your NLP algorithm in a **spreadsheet**\n",
        "- More NLP techniques\n",
        "- The Grand Challenge\n",
        "- Whatever you want\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx5Hpev0qHTy",
        "colab_type": "text"
      },
      "source": [
        "## 0.4. Consejos\n",
        "- **No** es necesario ejecutar el notebook a la vez que yo ni copiar todas las líneas de código que vaya añadiendo\n",
        "  - El `learning by doing` está muy bien, pero va más allá de escribir y ejecutar líneas de código contrarreloj sin saber lo que hacemos.\n",
        "  - Mejor limitarse primero a escuchar y tratar de **comprender** los conceptos\n",
        "  - Dejaré tiempo para que ejecutéis los notebook y podáis comprender y analizar los resultados entre todos\n",
        "\n",
        "  <center>\n",
        "<img src=https://i.gifer.com/1FA.gif width=\"500\">\n",
        "</center> \n",
        "\n",
        "- La programación es un **medio**, no un **fin**, para conseguir hacer Machine Learning (no es el único, ¿os he hablado de `BigML` ya?)\n",
        "  - Lo que hoy queremos aprender es cómo aplicar técnicas de Machine Learning a textos, no queremos aprender a programar los algoritmos que lo hagan\n",
        "  - Algo importante a recordar en ML: `No necesitamos reinventar la rueda cada vez`\n",
        "\n",
        "- Esta es la **última** sesión teórica\n",
        "  - Tenemos tiempo para que discutir sobre las puertas que nos puede abrir el Machine Learning, los problemas que le vemos, las dificultades en su implantación, etc\n",
        "  - Un **diálogo** es mejor que un **monólogo**: preguntad, proponed, **cuestionadme**, sed críticos\n",
        "\n",
        "   <center>\n",
        "<img src=https://i.gifer.com/ZPIA.gif width=\"500\">\n",
        "</center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK-nfXfiOfCx",
        "colab_type": "text"
      },
      "source": [
        "## 0.5 In action\n",
        "\n",
        "<center>\n",
        "<img src=https://i.gifer.com/3wv4.gif width=\"500\">\n",
        "</center> \n",
        "\n",
        "### 0.5.1. Sorry, we are not going to learn Neuro-linguistic programming\n",
        "- Open Google\n",
        "- Type \"`NLP`\"\n",
        "- You will see a lot of results about **Neuro-linguistic programming (NLP)**\n",
        "- Nothing to do with our NLP: **Natural Language Processing**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C55lRfFu09oV",
        "colab_type": "text"
      },
      "source": [
        "### 0.5.2 Last month in NLP...\n",
        "- Today is 16/11/2019\n",
        "\n",
        "<center>\n",
        "<img src=https://i.gifer.com/1y8s.gif width=\"500\">\n",
        "</center> \n",
        "\n",
        "#### 21/10/2019 - FB explains its new features to protect the 2020 US Elections\n",
        "  - Some of them using NLP techniques\n",
        "  - NLP helped fighting Voter Suppression and Intimidation\n",
        "  - [View more](https://newsroom.fb.com/news/2019/10/update-on-election-integrity-efforts/?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_129)\n",
        "  - ¿Por qué no aplican lo mismo al hate speech?\n",
        "  - ¿Y las fake news?\n",
        "\n",
        "#### 26/10/2019 -  Google explains BERT: Its last big release for the search engine\n",
        "\n",
        "- One of the biggest changes in the **search engine**\n",
        "- NLP to understand searchs better and offer better results\n",
        "  - From matching keywords to understanding the context of each word\n",
        "- [View more](https://www.blog.google/products/search/search-language-understanding-bert/)\n",
        "\n",
        "#### 05/11/2019 - GPT-2 Released\n",
        "  - See how a modern neural network completes your text\n",
        "  - Model released two weeks ago\n",
        "    - It can be **dangerous**? \n",
        "    [See this](https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction)\n",
        "  - [Talk to transformer](https://talktotransformer.com/)\n",
        "    - \"The most difficult part of Machine Learning is\"\n",
        "    - \"The 2019 spanish elections\"\n",
        "    - https://twitter.com/unmonoqueteclea/status/1193996445784911872"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5urIC3ExM7",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "You will need  **fastai 0.7.0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7t18hOzlr4L",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Installing FastAI Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T84NNbEPkVaw",
        "colab_type": "code",
        "outputId": "2f4a69c2-f1f0-4e70-a42a-90a0c5d9ea0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "print (\" Installing FastAI libraries...\")\n",
        "!pip install fastai==0.7.0 > /dev/null\n",
        "print (\"\\n Installing required libraries...\")\n",
        "!pip install torchtext==0.2.3 > /dev/null    # Corrects torch error, accepts Float32 type\n",
        "!git clone https://github.com/fastai/fastai.git fastai_ml\n",
        "!ln -s fastai_ml/courses/ml1/fastai/ fastai\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Installing FastAI libraries...\n",
            "\u001b[31mERROR: torchvision 0.4.2+cu100 has requirement torch==1.3.1, but you'll have torch 0.3.1 which is incompatible.\u001b[0m\n",
            "\n",
            " Installing required libraries...\n",
            "Cloning into 'fastai_ml'...\n",
            "remote: Enumerating objects: 31882, done.\u001b[K\n",
            "remote: Total 31882 (delta 0), reused 0 (delta 0), pack-reused 31882\u001b[K\n",
            "Receiving objects: 100% (31882/31882), 434.60 MiB | 39.41 MiB/s, done.\n",
            "Resolving deltas: 100% (23219/23219), done.\n",
            "Checking out files: 100% (815/815), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6yqIgPokeeL",
        "colab_type": "code",
        "outputId": "6c824bad-60f9-48cf-b7f9-78dd1da55dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\10/'    \n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "version='1.0.0'\n",
        "torch_url=f\"http://download.pytorch.org/whl/{accelerator}/torch-{version}-{platform}-linux_x86_64.whl\"\n",
        "!pip install -U {torch_url} torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.0.0\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu100/torch-1.0.0-cp36-cp36m-linux_x86_64.whl (753.6MB)\n",
            "\u001b[K     |████████████████████████████████| 753.6MB 96.1MB/s \n",
            "\u001b[?25hRequirement already up-to-date: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2+cu100)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "\u001b[31mERROR: torchvision 0.4.2+cu100 has requirement torch==1.3.1, but you'll have torch 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 0.3.1\n",
            "    Uninstalling torch-0.3.1:\n",
            "      Successfully uninstalled torch-0.3.1\n",
            "Successfully installed torch-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NbggViKlvjT",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Main imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AjXT1pJExNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "from fastai.nlp import *\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction import text "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_vZrd7cnMrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhCytW1rExNR",
        "colab_type": "text"
      },
      "source": [
        "## 2. IMDB dataset and the sentiment classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0a23jNyExNU",
        "colab_type": "text"
      },
      "source": [
        "The [large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB. The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews:\n",
        "\n",
        "  - A negative review has a score ≤ 4 out of 10\n",
        "  - A positive review has a score ≥ 7 out of 10. \n",
        "  - Neutral reviews are not included in the dataset. \n",
        "  \n",
        "The dataset is divided into training and test sets. \n",
        "The training set is the same 25,000 labeled reviews.\n",
        "\n",
        "The **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTgOerjInm4s",
        "colab_type": "text"
      },
      "source": [
        "### 2.0. Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn6GNYDbExNW",
        "colab_type": "code",
        "outputId": "aefe10a4-85b7-4d3e-abac-71fc2ce8de3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!mkdir data/\n",
        "!curl http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz  --output data/aclImdb_v1.tar.gz \n",
        "!gunzip data/aclImdb_v1.tar.gz;\n",
        "!tar -xvf data/aclImdb_v1.tar -C data/  > /dev/null"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  65.7M      0  0:00:01  0:00:01 --:--:-- 65.7M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "ngY0gh3SExNi",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Tokenizing and term document matrix creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD8lLoQO7Ohc",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.1 Understanding data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "yrYD2NU9ExNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH='data/aclImdb/'\n",
        "names = ['neg','pos']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "oucJrBTFExNq",
        "colab_type": "code",
        "outputId": "75849d4a-d016-4e52-979b-6b31aec1b09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls {PATH}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "3N-topY1ExNw",
        "colab_type": "code",
        "outputId": "5446a8be-4804-4676-bf39-15b56bef569c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%ls {PATH}train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  \u001b[0m\u001b[01;34mpos\u001b[0m/    unsupBow.feat  urls_pos.txt\n",
            "\u001b[01;34mneg\u001b[0m/             \u001b[01;34munsup\u001b[0m/  urls_neg.txt   urls_unsup.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlf0pZIz5kHz",
        "colab_type": "text"
      },
      "source": [
        "It seems that every review is within a different file.\n",
        "\n",
        "Knowing the label of a review is as easy as knowing its parent folder `pos`/`neg`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "a0HpDG3AExN3",
        "colab_type": "code",
        "outputId": "9edb1164-671d-47a1-ae4f-dc7343c3ab9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "%ls {PATH}train/pos | head"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0_9.txt\n",
            "10000_8.txt\n",
            "10001_10.txt\n",
            "10002_7.txt\n",
            "10003_8.txt\n",
            "10004_8.txt\n",
            "10005_7.txt\n",
            "10006_7.txt\n",
            "10007_7.txt\n",
            "10008_7.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "MwecaFKkExN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn,trn_y = texts_labels_from_folders(f'{PATH}train',names)\n",
        "val,val_y = texts_labels_from_folders(f'{PATH}test',names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "IlquN6HtExOC",
        "colab_type": "text"
      },
      "source": [
        "What does texts_labels_from_folders?\n",
        "Let's check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "vtf3mgfnExOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "??texts_labels_from_folders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "uEnxk-pzExOJ",
        "colab_type": "text"
      },
      "source": [
        "Here is the text of the first review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "9EABIYZ2ExOM",
        "colab_type": "code",
        "outputId": "d29a2112-78d2-4dd2-88d5-2d3bf8ed5f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "trn[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Oh, it's the movie - I thought I waited too long to take out the dog... I can't believe I watched the whole thing. I guess I was optimistically anticipating that it was going to get better. Horribly disjointed dialog, pathetic acting, and totally improbable events. Like Toby's mom hanging herself in the time it takes Col to walk upstairs and back down in a room with a 24' ceiling and no chairs, counters or anything around her motionlessly suspended body that she could have possibly used to climb on to do herself in. The little girl that played the daughter of the last family was the best actor in the whole movie, and the puppy of the first couple was a close second. The basic storyline has potential and with a good script and director could be a seriously creepy flick, but this version sadly is not it. I get more scared when I open my electric bill every month.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "snEZETKwExOQ",
        "colab_type": "code",
        "outputId": "b369eb27-6000-4f59-945a-b867266a0ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trn_y[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4iitTol7TGs",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.2. Creating feature vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "MXbBcABpExOW",
        "colab_type": "text"
      },
      "source": [
        "[`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts (part of `sklearn.feature_extraction.text`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "eulP6afIExOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "veczr = CountVectorizer(tokenizer=tokenize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98eT44ZhYNlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "??tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G1DdymLZDZj",
        "colab_type": "text"
      },
      "source": [
        "See more: https://github.com/fastai/fastai/blob/master/old/fastai/text.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "S-J3fAscExOc",
        "colab_type": "text"
      },
      "source": [
        "`fit_transform(trn)` finds the vocabulary in the training set. It also transforms the training set into a term-document matrix. \n",
        "\n",
        "Since we have to apply the *same transformation* to your validation set, the second line uses just the method `transform(val)`. \n",
        "\n",
        "`trn_term_doc` and `val_term_doc` are sparse matrices. `trn_term_doc[i]` represents training document i and it contains a count of words for each document for each word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "B2FrHXKcExOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_term_doc = veczr.fit_transform(trn)\n",
        "val_term_doc = veczr.transform(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "oJizty_DExOi",
        "colab_type": "text"
      },
      "source": [
        "**QUESTION 1**: What happens if some of the words in validation doesn't appear in training dataset?\n",
        "\n",
        "**ANSWER**: This tokenizers usually use a **unknown** token for these cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "25XY43pAExOm",
        "colab_type": "code",
        "outputId": "2e4ca31a-8b5f-4dc0-e7fb-baf5208ae4fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "trn_term_doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 3749745 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Vg_5L1NiExOq",
        "colab_type": "text"
      },
      "source": [
        "**QUESTION 2**: What does these dimensions mean? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "-5YcGkAFExOs",
        "colab_type": "text"
      },
      "source": [
        "We store it about a **sparse matrix**. \n",
        "It only stores non-zeros and it's more efficient for these kind of matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "DvN0o_QWExOv",
        "colab_type": "code",
        "outputId": "ebff2325-394d-4b9d-f081-6a0e4ad3e598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "trn_term_doc[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 117 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNBlylwp-MQi",
        "colab_type": "text"
      },
      "source": [
        "Each word is mapped to a number. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "GUsSC6wiExOz",
        "colab_type": "code",
        "outputId": "dd7ed534-5ad9-43ec-9acf-052a11a969e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = veczr.get_feature_names(); \n",
        "vocab[13000:13005]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['clustering', 'clutch', 'clutches', 'clutching', 'clutter']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "ED_ivyA_ExO3",
        "colab_type": "text"
      },
      "source": [
        "We can use `vocabulary_` to obtain the id of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "W42uBnNuExO5",
        "colab_type": "code",
        "outputId": "37cd3d51-597d-42e7-ee1b-91588e50291c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "veczr.vocabulary_['absurd']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1297"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "MdZURBUUExPN",
        "colab_type": "text"
      },
      "source": [
        "- 1297 is the `id` of the word `absurd`\n",
        "- 13000 is the `id` of the word `clustering`\n",
        "- **QUESTION 3**: What are we doing below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "91FMX7upExPO",
        "colab_type": "code",
        "outputId": "ba40c230-d5ec-47bd-bba9-f57777b7926c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trn_term_doc[0,1297]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "xrqacMr_ExPS",
        "colab_type": "code",
        "outputId": "fb4c0a7c-74f4-4b07-ae07-d784ae5657f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trn_term_doc[1,13000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "iOhzpHA1ExPq",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "rGtiT8oxExPs",
        "colab_type": "text"
      },
      "source": [
        "We can fit a logistic regression using the matrices calculated previously as features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afivluz6pJrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_accuracy(preds,ground):\n",
        "  accuracy = (preds==ground).sum()/preds.shape[0]\n",
        "  print(\"The accuracy is: {}%\".format(100*accuracy))\n",
        "  return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "XhMCmMO-ExPs",
        "colab_type": "code",
        "outputId": "e7ca9724-3ef0-40eb-e537-0587cf05e37b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m = LogisticRegression(dual=True)\n",
        "m.fit(trn_term_doc, trn_y)\n",
        "preds = m.predict(val_term_doc)\n",
        "accuracy = print_accuracy(preds,val_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is: 87.148%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVxlxKbB-JS",
        "colab_type": "text"
      },
      "source": [
        "- **QUESTION 4**: What are we doing below to increase accuracy?\n",
        "  - Test np.sign() with different numbers: -4, -2, 1 3, 10, etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "RXeE67kuExPw",
        "colab_type": "code",
        "outputId": "8209da89-5945-41c8-8b92-ef3670fc8203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m = LogisticRegression(dual=True)\n",
        "m.fit(trn_term_doc.sign(), trn_y)\n",
        "preds = m.predict(val_term_doc.sign())\n",
        "accuracy = print_accuracy(preds,val_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is: 87.384%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqfwRle7A1aZ",
        "colab_type": "code",
        "outputId": "1f11e020-5ce3-4ced-dfdc-cf74c41cafba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "help(val_term_doc.sign)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method sign in module scipy.sparse.data:\n",
            "\n",
            "sign() method of scipy.sparse.csr.csr_matrix instance\n",
            "    Element-wise sign.\n",
            "    \n",
            "    See numpy.sign for more information.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "IkWNE9okExPy",
        "colab_type": "text"
      },
      "source": [
        "We can play with **regularization** to avoid overfitting...\n",
        "\n",
        "See [04:49]: https://www.coursera.org/lecture/ml-classification/visualizing-effect-of-l2-regularization-in-logistic-regression-1VXLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "vH8_FUdNExP0",
        "colab_type": "code",
        "outputId": "1bcc4a57-7481-427d-d17d-7cf67c3cfbd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(trn_term_doc, trn_y)\n",
        "preds = m.predict(val_term_doc)\n",
        "accuracy = print_accuracy(preds,val_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is: 88.28%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "mi38wsnLExP2",
        "colab_type": "code",
        "outputId": "79080820-d217-4a58-d6cc-e84629f97028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(trn_term_doc.sign(), trn_y)\n",
        "preds = m.predict(val_term_doc.sign())\n",
        "accuracy = print_accuracy(preds,val_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is: 88.40400000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xCb2Vb2Cw_H",
        "colab_type": "text"
      },
      "source": [
        "88.4% of accuracy!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJLLiBxtCs9Z",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://media.giphy.com/media/xT0GqssRweIhlz209i/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckFN0ro3-jpU",
        "colab_type": "text"
      },
      "source": [
        "### 2.3. Your NLP model in a spreadsheet\n",
        "\n",
        "See here:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1hZcfAKBp4BqvefEFk0pOhbHxVb7sX7tz6QxbnUL8kSE/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgrX08rFbeHE",
        "colab_type": "text"
      },
      "source": [
        "![link text](https://media.giphy.com/media/SYQIWpavmTyta4nQhK/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "NwIN1bKzExPW",
        "colab_type": "text"
      },
      "source": [
        "### 2.4. Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "H2BUjX7XExPW",
        "colab_type": "text"
      },
      "source": [
        "We define the **log-count ratio** $r$ for each word $f$:\n",
        "\n",
        "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n",
        "\n",
        "where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents.\n",
        "\n",
        "It's nice to use the log because you can sum things together instead multiplying"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "8gOxt42fExPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The probability of a word given a class\n",
        "def pr(x,y,y_i):\n",
        "    p = x[y==y_i].sum(0)\n",
        "    return (p+1) / ((y==y_i).sum()+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "13jqUojrExPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ratios (for each word) between the probability for the\n",
        "# class 1 and the probability for the class 0\n",
        "r = np.log(pr(trn_term_doc, trn_y,1)/pr(trn_term_doc, trn_y,0))\n",
        "b = np.log((trn_y==1).mean() / (trn_y==0).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "te55DIXYExPf",
        "colab_type": "text"
      },
      "source": [
        "Here is the formula for Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "m4HThjvhExPh",
        "colab_type": "code",
        "outputId": "d3021464-144a-48ff-e5d8-5d6b0e4ebc30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pre_preds = val_term_doc @ r.T + b\n",
        "preds = pre_preds.T>0\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.81656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "2wcQgoSRExPl",
        "colab_type": "text"
      },
      "source": [
        "...and binarized Naive Bayes.\n",
        "\n",
        "`.sign` replaces everything positive by a 1 and everything negative (or zero) by a 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "p8C3Gjj9ExPn",
        "colab_type": "code",
        "outputId": "6708d6a7-daaa-4520-cd37-40a4149387a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pre_preds = val_term_doc.sign() @ r.T + b\n",
        "preds = pre_preds.T>0\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we4OY0MnExP5",
        "colab_type": "text"
      },
      "source": [
        "### 2.5. Trigram with NB features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN-kiMzhExP7",
        "colab_type": "text"
      },
      "source": [
        "Our next model is a version of logistic regression with Naive Bayes features described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNdvRO7iExP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\n",
        "trn_term_doc = veczr.fit_transform(trn)\n",
        "val_term_doc = veczr.transform(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i2JDEEcExP9",
        "colab_type": "code",
        "outputId": "2f0bf63e-f148-4128-b166-42727b7596eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trn_term_doc.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 800000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkaj8Pf-ExQM",
        "colab_type": "text"
      },
      "source": [
        "Here we fit regularized logistic regression where the features are the trigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY1TZ1qoExQM",
        "colab_type": "code",
        "outputId": "f104a215-2d52-412a-804f-8c59d2a8e9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(trn_term_doc.sign(), trn_y);\n",
        "preds = m.predict(val_term_doc.sign())\n",
        "accuracy = print_accuracy(preds,val_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is: 90.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VX60i7ErR79",
        "colab_type": "text"
      },
      "source": [
        "Let's use **log_count_ratio** as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aoKFSRRExQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr(trn_term_doc, trn_y,1) / pr(trn_term_doc, trn_y,0))\n",
        "b = np.log((trn_y==1).mean() / (trn_y==0).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jAuRnLjExQQ",
        "colab_type": "text"
      },
      "source": [
        "Here is the $\\text{log-count ratio}$ `r`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMJiyTuRExQR",
        "colab_type": "code",
        "outputId": "95142ef6-4638-4b1e-bbaa-5dcf4771f5de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "r.shape, r"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 800000),\n",
              " matrix([[-0.07511, -0.00034,  0.10548, ...,  1.38629, -2.07944, -2.07944]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzxjajOPExQa",
        "colab_type": "text"
      },
      "source": [
        "Here we fit regularized logistic regression where the features are the trigrams' log-count ratios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_gEkHfuExQb",
        "colab_type": "code",
        "outputId": "798b3424-d696-4c76-aca2-05a3bbaf1abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_nb = trn_term_doc.multiply(r)\n",
        "m = LogisticRegression(dual=True, C=0.1)\n",
        "m.fit(x_nb, trn_y);\n",
        "\n",
        "val_x_nb = val_term_doc.multiply(r)\n",
        "preds = m.predict(val_x_nb)\n",
        "accuracy = print_accuracy(preds,val_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is: 91.556%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M51hUwVExQe",
        "colab_type": "text"
      },
      "source": [
        "### 2.6. fastai NBSVM++"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u40i2e-ExQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sl=2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4p_ZQ6VExQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is how we get a model from a bag of words\n",
        "md = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVQapHU6ExQk",
        "colab_type": "code",
        "outputId": "1f291453-73aa-4685-b87b-908bbf9d987c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "learner = md.dotprod_nb_learner()\n",
        "learner.fit(0.02, 1, wds=1e-6, cycle_len=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a615d085da7040609bdb2bf4cb42fe90",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   <lambda>   \n",
            "    0      0.024123   0.119538   0.91672   \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11953848763465881, 0.9167200000190735]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxh95PrIExQm",
        "colab_type": "code",
        "outputId": "1a0a1f21-7117-4072-a69c-8ebfdb007ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "296c43bf184b484d8334b2c8a8a73355",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   <lambda>   \n",
            "    0      0.020608   0.113216   0.92176   \n",
            "    1      0.011949   0.111858   0.92132   \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11185849909305573, 0.9213200000572205]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEuSaIrtExQp",
        "colab_type": "code",
        "outputId": "9927cd8f-d1e4-427c-c06f-1c5b062d2951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8b8ed283cd444dfbd717d70bd4a9d55",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch      trn_loss   val_loss   <lambda>   \n",
            "    0      0.016833   0.110698   0.92204   \n",
            "  7%|▋         | 27/391 [00:00<00:07, 47.18it/s, loss=0.014] "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8pP_FB7yHcC",
        "colab_type": "text"
      },
      "source": [
        "# 3. More NLP techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzQOTuiYyLPI",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Stop words remove\n",
        "\n",
        "- Very useful when the stop words appear more in one of the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6av_yZa-yPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(text.ENGLISH_STOP_WORDS)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_sOYtGQ9v1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "veczr =  CountVectorizer(ngram_range=(1,3), \n",
        "                         tokenizer=tokenize, \n",
        "                         max_features=800000,\n",
        "                         stop_words=\"english\")\n",
        "trn_term_doc = veczr.fit_transform(trn)\n",
        "val_term_doc = veczr.transform(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZimydDNmyNu4",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Stemming and Lemmatizing\n",
        "\n",
        "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js4AAeHDQW5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language=\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E14XcTOvQY63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(stemmer.stem(\"foot\"))\n",
        "print(stemmer.stem(\"feet\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_tiSv_TSNCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(stemmer.stem(\"play\"))\n",
        "print(stemmer.stem(\"plays\"))\n",
        "print(stemmer.stem(\"played\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8xgJ7nFRHTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmer=WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3FW0SLkRJ-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lemmer.lemmatize(\"foot\"))\n",
        "print(lemmer.lemmatize(\"feet\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEVCNkVISbzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lemmer.lemmatize(\"play\" ))\n",
        "print(lemmer.lemmatize(\"plays\"))\n",
        "print(lemmer.lemmatize(\"played\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zL05glmS2he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lemmer.lemmatize(\"play\", pos='v' ))\n",
        "print(lemmer.lemmatize(\"plays\", pos='v'))\n",
        "print(lemmer.lemmatize(\"played\", pos='v'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRiBxGkSSsjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lemmer.lemmatize(\"are\", pos='v' ))\n",
        "print(lemmer.lemmatize(\"is\", pos='v'))\n",
        "print(lemmer.lemmatize(\"being\", pos='v'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXPFWaYLTAJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming_tokenizer(str_input):\n",
        "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5FbSk52TCnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv =  CountVectorizer(ngram_range=(1,3), tokenizer=stemming_tokenizer, max_features=800000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17imBOX5TR1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_term_doc = cv.fit_transform(trn)\n",
        "val_term_doc = cv.transform(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6KBdY-9TdR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(trn_term_doc, trn_y);\n",
        "preds = m.predict(val_term_doc)\n",
        "accuracy = print_accuracy(preds,val_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv9sxHwx_LWu",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 TF-IFD\n",
        "\n",
        "TF-IDF (stands for Term-Frequency-Inverse-Document Frequency) weights down the common words occuring in almost all the documents and give more importance to the words that appear in a subset of documents. TF-IDF works by penalising these common words by assigning them lower weights while giving importance to some rare words in a particular document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOtm1P7R_CBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=False,use_idf=False)\n",
        "trn_term_doc_tfidf = tfidf_transformer.fit_transform(trn_term_doc)\n",
        "val_term_doc_tfidf = tfidf_transformer.transform(val_term_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8hzwltJCkwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(trn_term_doc_tfidf, trn_y);\n",
        "preds = m.predict(val_term_doc_tfidf)\n",
        "accuracy = print_accuracy(preds,val_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PokRK9KvyTDa",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. Word embeddings\n",
        "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
        "\n",
        "http://vectors.nlpl.eu/explore/embeddings/en/calculator/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prBuw9jMye4D",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Deep Learning\n",
        "### 3.5.1 Recurrent Neural Networks\n",
        "  - https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "  - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "### 3.5.2 Attention networks\n",
        "  - https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f\n",
        "#### 3.5.3 Transformers\n",
        "  - https://towardsdatascience.com/transformers-141e32e69591"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQFFq5SmFrXi",
        "colab_type": "text"
      },
      "source": [
        "# 4. The Grand Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D52yGXkRFueK",
        "colab_type": "text"
      },
      "source": [
        "### 4.1. Dataset: 20 News Groups\n",
        "\n",
        "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKkKlZJOGnIt",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.1 Setup\n",
        "\n",
        "Loading the **train** and **test** datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrjigPkpGiNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "test = fetch_20newsgroups(subset='test', shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc-N3xfPHDeE",
        "colab_type": "text"
      },
      "source": [
        "These are the possible classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePNwapEzGq7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.target_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXt9rLmzH3nY",
        "colab_type": "text"
      },
      "source": [
        "Let's see a random instance of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeywhOsbHKNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train.data[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NETHl1WTIt3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"There are {} instance in the train dataset\".format(len(train.data)))\n",
        "print(\"There are {} instance in the test dataset\".format(len(test.data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj67OJO9IIda",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.2 Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DN5o3VKMK3D",
        "colab_type": "text"
      },
      "source": [
        "Let's extract features from the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S2ocUf3Idh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "train_counts = count_vect.fit_transform(train.data)\n",
        "test_counts = count_vect.transform(test.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsNd8qm1JCcr",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.3 Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on7TGAoEMSsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(train_counts, train.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bObMgeaKJR5U",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1.4 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgxPJPGxJhsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "predicted = clf.predict(test_counts)\n",
        "print(\"Accuracy = {}%\".format(100*np.mean(predicted == test.target)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "ezvHsteYExQr",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "AiC4NauzExQs",
        "colab_type": "text"
      },
      "source": [
        "* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https://www.aclweb.org/anthology/P12-2018)"
      ]
    }
  ]
}